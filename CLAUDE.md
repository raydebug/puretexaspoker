CLAUDE.md â€“ Continuous Improvement Workflow

Claude must follow this structured, iterative workflow to ensure clarity, reusability, traceability, and continuous quality improvement across all development and testing activities.

This workflow mandates controlled test updates, screenshot-based verification, result tracking, and audit-friendly documentation.

â¸»

ğŸŒ€ Iteration Workflow

1. Summarize the Target of This Interaction

At the beginning of each task or session, Claude must clearly state the objective.
Examples:
   â€¢  â€œFix login timeout logicâ€
   â€¢  â€œAdd input validation for registration formâ€
   â€¢  â€œRefactor invoice rendering flowâ€

â¸»

2. Scan for Reuse Before Code/Test Changes

Before writing or editing any code or test:
   â€¢  Review the entire project to avoid redundancy.
   â€¢  Reuse existing:
   â€¢  Functions / utilities
   â€¢  Constants / configs
   â€¢  Tests / mocks / assertions
   â€¢  Refer to or maintain PROJECT_OVERVIEW.md, which describes:
   â€¢  Purpose of each source/test file
   â€¢  Key methods and their behavior
   â€¢  Constant definitions
   â€¢  Test coverage per feature
   â€¢  Shared components/utilities

â¸»

3. Identify Issue or Plan the Next Design

Clearly define the bug, task, or feature to address in this iteration.

â¸»

4. Implement or Fix Code

Apply changes only after confirming reuse opportunities have been exhausted.

â¸»

â— Test Modification Policy

Claude must strictly follow these rules:
   â€¢  Do not change, remove, or bypass existing tests without explicit permission.
   â€¢  Add or extend tests only when functional logic changes.
   â€¢  If a test appears outdated or invalid, flag it for review â€” but do not modify it directly.

â¸»

5. Update Tests as Needed (With Permission)
   â€¢  For backend/API changes, update or add backend unit/integration tests.
   â€¢  For UI/frontend changes, update or add Selenium UI test cases.
   â€¢  Test changes must follow the permission rule above.

â¸»

6. Run UI Tests with Screenshot Verification

âœ… Screenshot Log File Naming
   â€¢  For every .feature file (e.g., checkout_flow.feature), create a single corresponding screenshot log file:

checkout_flow_screenshots.md


   â€¢  This naming pattern is mandatory.

ğŸ“¸ Screenshot Log File Format
   â€¢  Before each test run, delete all previous screenshots.
   â€¢  During testing, capture screenshots of each UI step.

Each run must update the associated screenshot log file, including:

# Screenshot Verification Log for `checkout_flow.feature`
**Test Run Time:** 2025-07-28 14:53:21

| Index | Screenshot File       | Verifying Items                           | Result   |
|-------|------------------------|-------------------------------------------|----------|
| 1     | checkout_step1.png     | Product page loaded                       | âœ… Pass  |
| 2     | checkout_step2.png     | Address form filled correctly             | âœ… Pass  |
| 3     | checkout_step3.png     | Payment confirmation visible              | âŒ Fail  |

ğŸ”’ Maintenance Rules
   â€¢  Never delete previously passed steps from the log.
   â€¢  Always update the Result column for each test run.
   â€¢  Always record the current timestamp (Test Run Time) for each test run.

â¸»

7. Track Test Coverage and Result Changes (Per Run)

After each test run, compare current vs. previous test metrics:

| Test Suite     | Prev Count | Curr Count | Î”Cases | Prev Pass % | Curr Pass % | Î”Pass % |
|----------------|------------|------------|--------|-------------|-------------|---------|
| Backend Tests  | 120        | 125        | +5     | 98%         | 99%         | +1%     |
| UI Tests       | 60         | 60         | 0      | 95%         | 93%         | -2%     |


â¸»

8. ğŸ“Š Maintain Test Results History File

Maintain a centralized test history log file named:

test_results_history.md

For each iteration, append a new record with:
   â€¢  Timestamp
   â€¢  Summary of test suites executed
   â€¢  Total cases, passes, fails, and pass rate
   â€¢  Key notes if any failures or regressions occurred

Example:

## Test Run â€“ 2025-07-28 14:53:21

| Suite         | Total | Passed | Failed | Pass % |
|---------------|-------|--------|--------|--------|
| Backend       | 125   | 124    | 1      | 99.2%  |
| UI            | 60    | 56     | 4      | 93.3%  |

- âœ… API refactor tests passed
- âŒ UI issue on checkout button (regression from v1.4.3)

This file must be kept up-to-date after every test run and serves as the basis for tracking long-term quality trends.

â¸»

9. Verify All Tests Pass

Run all relevant tests:
   â€¢  âœ… If all backend and UI tests pass (including screenshot verification), continue.
   â€¢  âŒ If anything fails, return to Step 4 to revise.

â¸»

10. Commit Changes

Once verified:
   â€¢  Commit all code, updated tests, screenshot logs, and the test history record.
   â€¢  Use a clear and descriptive commit message.

â¸»

11. Repeat

Return to Step 1 and continue improving.
Claude must treat this process as a continuous improvement cycle.

â¸»

ğŸ“˜ PROJECT_OVERVIEW.md (Recommended)

Claude may refer to or maintain a PROJECT_OVERVIEW.md that includes:

Section  Content
Files Purpose of each code and test file
Functions   Responsibilities of key methods
Constants   Global values and where they are used
Tests Coverage mapping from features to test cases
Utilities   Reusable helpers, validators, etc.