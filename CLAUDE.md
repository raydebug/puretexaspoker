CLAUDE.md ‚Äî Continuous Improvement (Short Version)

Claude must follow this iterative workflow in every interaction to ensure clarity, reuse, and verifiable quality.

1) State the Goal
   ‚Ä¢  Begin by summarizing the target of this interaction (bug/feature/design).

2) Reuse First
   ‚Ä¢  Scan the whole project before any change to avoid duplication and reuse existing methods, constants, helpers, and tests.
   ‚Ä¢  Maintain (or consult) PROJECT_OVERVIEW.md describing files, key methods, tests, constants, shared utilities, and reusable resources.

3) Plan ‚Üí Implement
   ‚Ä¢  Identify the issue or next design.
   ‚Ä¢  Implement or fix code after considering reuse.

4) Test Modification Policy (Strict)
   ‚Ä¢  Do not change, remove, or bypass existing tests without explicit permission.
   ‚Ä¢  Add/extend tests only when logic changes; otherwise leave tests unchanged.

5) Update & Run Tests
   ‚Ä¢  Backend/API changes ‚Üí update/run backend tests.
   ‚Ä¢  UI changes ‚Üí update/run Selenium UI tests.

Browser Reuse Policy for UI Tests

Action   When to Reuse Existing Browser Instance   When to Restart Browser
UI Test Session   Default ‚Äî all UI tests in same suite   Only if browser is unstable or tests fail due to session state
Memory Management When resource usage is within limits   If memory leaks cause slowdown or crashes
Session Handling  When session cookies/tokens remain valid  If login/session expired and cannot be restored programmatically
Performance If startup time is significant   If cold start is required for clean environment

   ‚Ä¢  Always prefer reusing existing browser sessions unless one of the restart conditions above applies.
   ‚Ä¢  Run all relevant suites; if anything fails, return to Step 3.

6) UI Screenshot Verification (Evidence on All Outcomes)
   ‚Ä¢  One-to-one log file per feature: if x_y_z.feature then only x_y_z_screenshots.md.
   ‚Ä¢  Before run: delete all old screenshots.
   ‚Ä¢  During run per step:
   ‚Ä¢  Perform verification.
   ‚Ä¢  Take a screenshot for both pass and fail results as evidence.
   ‚Ä¢  Screenshot log file must always reflect the feature steps and include:
   ‚Ä¢  Index | Screenshot File | Verifying Items | Result
   ‚Ä¢  Never remove previously passed entries.
   ‚Ä¢  Add ‚ÄúTest Run Time: ‚Äù for each run.

7) Test Metrics Tracking
   ‚Ä¢  After each run, compare current vs previous:
   ‚Ä¢  Test case count, pass/fail counts, pass rate (%).
   ‚Ä¢  Summarize deltas (‚Üë/‚Üì) to highlight improvements or regressions.

8) Test Results History (Central Log)
   ‚Ä¢  Append each run to test_results_history.md with:
   ‚Ä¢  Timestamp
   ‚Ä¢  Per-suite totals (Total/Passed/Failed/Pass %)
   ‚Ä¢  Brief notes on regressions/improvements.

9) Commit & Iterate
   ‚Ä¢  Commit only after all checks pass (including screenshot rules and logs).
   ‚Ä¢  Use clear commit messages.
   ‚Ä¢  Return to Step 1 and repeat.

‚∏ª

üìå Notes
   ‚Ä¢  Keep PROJECT_OVERVIEW.md, all *_screenshots.md, and test_results_history.md current.
   ‚Ä¢  Enforce exact naming:

feature_name.feature ‚Üî feature_name_screenshots.md



‚∏ª

üìé Appendix

Example: x_y_z_screenshots.md

# Screenshot Verification Log for `x_y_z.feature`
**Test Run Time:** 2025-08-13 10:32:15

| Index | Screenshot File   | Verifying Items                     | Result   |
|-------|-------------------|--------------------------------------|----------|
| 1     | step1_loaded.png  | Main page loaded, title visible      | ‚úÖ Pass  |
| 2     | step2_input.png   | Form fields filled with valid data   | ‚úÖ Pass  |
| 3     | step3_error.png   | Error message displayed              | ‚ùå Fail  |


‚∏ª

Example: test_results_history.md

## Test Run ‚Äî 2025-08-13 10:32:15

| Suite         | Total | Passed | Failed | Pass % |
|---------------|-------|--------|--------|--------|
| Backend       | 125   | 124    | 1      | 99.2%  |
| UI            | 60    | 59     | 1      | 98.3%  |

- ‚úÖ Backend API improvements confirmed stable.
- ‚ùå UI: Error message step failed in `x_y_z.feature`.


‚∏ª

üÜï Empty Template ‚Äî feature_name_screenshots.md

# Screenshot Verification Log for `feature_name.feature`
**Test Run Time:** YYYY-MM-DD HH:MM:SS

| Index | Screenshot File   | Verifying Items                     | Result   |
|-------|-------------------|--------------------------------------|----------|
| 1     |                   |                                      |          |
| 2     |                   |                                      |          |
| 3     |                   |                                      |          |


‚∏ª

üÜï Empty Template ‚Äî test_results_history.md

## Test Run ‚Äî YYYY-MM-DD HH:MM:SS

| Suite         | Total | Passed | Failed | Pass % |
|---------------|-------|--------|--------|--------|
| Backend       |       |        |        |        |
| UI            |       |        |        |        |

- Notes:
  - 
